---
title: "Appendix H: Installing the PSM on a KVM Cluster"
menu:
  docs:
    parent: "PSM Enterprise Edition User Guide"
weight: 27
categories: [psm]
toc: true
---
## Appendix H: Installing the PSM on a KVM Cluster
This section provides a more detailed guide to the PSM installation procedure on KVM hosts. In the example shown in Figure 68, three servers, running CentOS 7.6 with KVM support, each hosts one of three PSM cluster nodes. (Pensando recommends that the three PSM cluster nodes should be distributed across multiple servers). Server requirements are specified in the document *Enterprise PSM Design Best Practices*.
  
![image alt text](/images/PSM/PSM_User_Guide/Appendix_H-_Installing_the_PSM_on_a_KVM_Cluster/f1bcc082ab1cb9def88887e545f4734acaf748f0.png)
<div style="text-align:center"><font size='2'>*Figure 68. A recommended PSM cluster configuration*
</font>

</div>In this example, the network configuration is assumed to be:

- <font size='2'>VLAN 5: Management Network → 10.29.5.0/24</font>
       - <font size='2'>Server1: 10.29.5.103</font>
       - <font size='2'>Server2: 10.29.5.104</font>
- <font size='2'>VLAN 11:  PSM Management Network → 10.29.11.0/24</font>
       - <font size='2'>PSM-N1: 10.29.11.21</font>
       - <font size='2'>PSM-N2: 10.29.11.22</font>
       - <font size='2'>PSM-N3: 10.29.11.23</font>
  
### Networking Configuration
Next, configure the needed network bridges for access to the CentOS server and for the PSM. The Top-of-Rack switch is configured to be a trunk interface that has set the “native-vlan” for both servers to be on VLAN 5 that also allows VLAN 11 to traverse the interface. Figure 69 shows how to create bridge interfaces that will be used by the virtual machine running a PSM node. Since in this environment there is only one network interface, create a bridge interface that will allow communication to the CentOS server.

  
![image alt text](/images/PSM/PSM_User_Guide/Appendix_H-_Installing_the_PSM_on_a_KVM_Cluster/44814255e8d79d1b05dcc2c0e1d1ed4cddb2b179.png)<div style="text-align:center"><font size='2'>
</font>

<font size='2'>*Figure 69.*
</font> 
<font size='2'>*Bridge interface for a PSM node*
</font>

</div>
##### Network Interface Configuration
Make sure the VLAN module is loaded for this server, with the command:


```
[root]# modprobe --first-time 8021q
modprobe: ERROR: could not insert '8021q': Module already in kernel
  
  

```

If the above error message is returned, then the VLAN tagging module was already loaded.  If there is no error message, the module has now been successfully loaded.
Create a bridge interface named `br0` that links interface `eno5`, which is an active interface on the server. This is done by modifying interface `eno5` in the configuration file `/etc/sysconfig/network-scripts/ifcfg-eno5`:


```
[root@server1]# cat /etc/sysconfig/network-scripts/ifcfg-eno5
# Generated by dracut initrd
NAME="eno5"
DEVICE="eno5"
ONBOOT=yes
NETBOOT=yes
UUID="ee73d6e6-60a2-4239-b21c-036c00cd6493"
IPV6INIT=yes
BOOTPROTO=static
TYPE=Ethernet
#IPADDR=10.29.5.103
#GATEWAY=10.29.5.1
#NETMASK=255.255.255.0
#DNS1=10.29.5.9
#DNS2=8.8.8.8
#DOMAIN=training.local
BRIDGE=br0
  
  

```

Note that the IP address, gateway, netmask, DNS, and domain entries are commented out, and the entry `BRIDGE=br0` is added.  This will link the `br0` interface to `eno5`.
<font size='2'>Create the configuration file
</font> 
`/etc/sysconfig/network-scripts/ifcfg-br0`<font size='2'> for interface
</font> 
`br0`<font size='2'>:
</font>



```
[root@server1]# cat /etc/sysconfig/network-scripts/ifcfg-br0 
DEVICE=br0
TYPE=Bridge
BOOTPROTO=static
IPADDR=10.29.5.103
NETMASK=255.255.255.0
GATEWAY=10.29.5.1
DNS1=10.29.5.9
DNS2=8.8.8.8
DOMAIN=training.local
ONBOOT=yes
DELAY=0
  
  

```
Now that these configuration files have been entered, run the following command to restart the network and verify that you can access the CentOS system server1. 


```
[root@server1]# systemctl restart network
  
  

```

***Note:*** *You may lose network connectivity and need to re-log back into the server.*
Log back into the server and check/verify the network interfaces.


```
[root@server1]# ip addr show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: eno5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq master br0 state UP group default qlen 1000
    link/ether 48:df:37:8f:d5:c8 brd ff:ff:ff:ff:ff:ff
    inet6 fe80::4adf:37ff:fe8f:d5c8/64 scope link 
       valid_lft forever preferred_lft forever
3: eno6: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether 48:df:37:8f:d5:c9 brd ff:ff:ff:ff:ff:ff
4: eno7: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether 48:df:37:8f:d5:ca brd ff:ff:ff:ff:ff:ff
5: eno8: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether 48:df:37:8f:d5:cb brd ff:ff:ff:ff:ff:ff
6: br0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 48:df:37:8f:d5:c8 brd ff:ff:ff:ff:ff:ff
    inet 10.29.5.103/24 brd 10.29.5.255 scope global br0
       valid_lft forever preferred_lft forever
    inet6 fe80::4adf:37ff:fe8f:d5c8/64 scope link 
       valid_lft forever preferred_lft forever
7: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000
    link/ether 52:54:00:8c:e5:f9 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0
       valid_lft forever preferred_lft forever
8: virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc pfifo_fast master virbr0 state DOWN group default qlen 1000
    link/ether 52:54:00:8c:e5:f9 brd ff:ff:ff:ff:ff:ff
  
  

```

***Note:***  *Now interface* `br0`*, not* `eno5`*, has the CentOS server1 management IP address.*
Next, create the interfaces that will be used for the PSM, that will reside on VLAN 11.  Create the interface that is linked to the physical interface, which in this example is `eno5`.  The syntax to create an interface with a VLAN tag is `interface.VLANID`.  In this example, the interface file will be `eno5.11`.   This interface will be bridged to the bridge interface, (called `br11` in this example).  Create these two network interface files in the directory `/etc/sysconfig/network-scripts/` .
The filenames will be `ifcfg-eno5.11` and `ifcfg-br11`.


```
[root@server1]# cat /etc/sysconfig/network-scripts/ifcfg-eno5.11 
DEVICE=eno5.11
BOOTPROTO=none
ONBOOT=yes
USERCTL=no
VLAN=yes
BRIDGE=br11
NM_CONTROLLED=no
DELAY=0
  
  


```
***Note:***  *For this example, the* `VLAN` *entry must be set to* `yes` *and the bridge interface should point to the* `br11` *interface.*



```
[root@server1]# cat /etc/sysconfig/network-scripts/ifcfg-br11 
DEVICE=br11
TYPE=Bridge
ONBOOT=yes
BOOTPROTO=none
NM_CONTROLLED=no
DELAY=0
  
  

```

Once these two files are created, restart the network service.


```
[root@server1]# systemctl restart network
  
  


```
Check the interfaces for the server:


```
[root@server1]# ip addr show
  
  

```

The interfaces `eno5.11` and `br11` have now been created.  Interface `br11` will be used for the PSM virtual machines.
You can also run the following command to check the bridge interfaces:


```
[root@server1]# brctl show
bridge name	bridge id		STP enabled	interfaces
br0		8000.48df378fd5c8	no		eno5
br11		8000.48df378fd5c8	no		eno5.11
virbr0	8000.5254008ce5f9	yes		virbr0-nic
  
  

```

Repeat the above steps to deploy the second and third servers.
### Installation of PSM Nodes
Now that the servers are prepared with the appropriate resources and network configurations, install the PSM software:

- Deploy each PSM node
- Set networking for the nodes
- Bootstrap the PSM cluster
  
To deploy the PSM, first download the `psm.qcow2` image to the CentOS server.  In this example, PSM node 1 will be deployed on server1, PSM node 2 will be deployed on server2 and PSM node 3 will be deployed on server3.  Each PSM node will require its own image, even though these images are copies of the same file.
### Deploying PSM Nodes
<font size='2'>In this example, a directory is created on the servers under
</font> 
`/root/kvm/images/1.7.0-9`<font size='2'>.  Follow the steps below to deploy the PSM nodes.
</font>

1. Copy the file `psm.qcow2` onto server1, server2 and server3
1. Copy `psm.qcow2` to a new image file on server 1→ ex: pod01-psm-n1
1. Copy `psm.qcow2` to a new image file on server 2 → ex: pod01-psm-n2
1. Copy `psm.qcow2` to a new image file on server3 → ex: pod01-psm-n3
  
Since the CentOS server will be used by the user root, modify qemu.conf file so the user and group root are allowed.  Edit the file` /etc/libvirtd/qemu.conf` and uncomment the two entries `user = “root”` and `group = “root”`, as below:


```
# Some examples of valid values are:
#
#       user = "qemu"   # A user named "qemu"
#       user = "+0"     # Super user (uid=0)
#       user = "100"    # A user named "100" or a user with uid=100
#
user = "root"

# The group for QEMU processes run by the system instance. It can be
# specified in a similar way to user.
group = "root"
  
  

```

Once the file is saved, restart the libvirtd service:


```
[root@server1]# systemctl restart libvirtd
  
  

```

Deploy the PSM nodes.
Server1:


```
[root@server1]# cd kvm/images/1.7.0-9/
[root@server1 1.7.0-9]# pwd
/root/kvm/images/1.7.0-9
[root@server1 1.7.0-9]# ls -al
total 2472704
drwxr-xr-x 2 root root         26 Apr  5 14:47 .
drwxr-xr-x 3 root root         21 Apr  5 13:14 ..
-rw-r--r-- 1 root root 2532048896 Apr  5 13:25 psm.qcow2
[root@server1 1.7.0-9]# cp psm.qcow2 pod01-psm-n1.qcow2
[root@server1 1.7.0-9]# ls -al
total 7418112
drwxr-xr-x 2 root root         78 Apr  5 14:43 .
drwxr-xr-x 3 root root         21 Apr  5 12:56 ..
-rw-r--r-- 1 root root 2532048896 Apr  5 14:41 pod01-psm-n1.qcow2
-rw-r--r-- 1 root root 2532048896 Apr  5 13:41 psm.qcow2
  
  


```
Server2:


```
[root@server2]# cd kvm/images/1.7.0-9/
[root@server2]# pwd
/root/kvm/images/1.7.0-9
[root@server2]# ls -al
total 2472704
drwxr-xr-x 2 root root         26 Apr  5 14:47 .
drwxr-xr-x 3 root root         21 Apr  5 13:14 ..
-rw-r--r-- 1 root root 2532048896 Apr  5 13:25 psm.qcow2
[root@server2]# cp psm.qcow2 pod01-psm-n2.qcow2
[root@server2]# ls -al
total 7418112
drwxr-xr-x 2 root root         78 Apr  5 14:43 .
drwxr-xr-x 3 root root         21 Apr  5 12:56 ..
-rw-r--r-- 1 root root 2532048896 Apr  5 16:20 pod01-psm-n1.qcow2
-rw-r--r-- 1 root root 2532048896 Apr  5 14:53 psm.qcow2
  
  


```
Server3:


```
[root@server3]# cd kvm/images/1.7.0-9/
[root@server3]# pwd
/root/kvm/images/1.7.0-9
[root@server3]# ls -al
total 2472704
drwxr-xr-x 2 root root         26 Apr  5 14:47 .
drwxr-xr-x 3 root root         21 Apr  5 13:14 ..
-rw-r--r-- 1 root root 2532048896 Apr  5 13:25 psm.qcow2
[root@server3]# cp psm.qcow2 pod01-psm-n3.qcow2
[root@server3]# ls -al
total 7418112
drwxr-xr-x 2 root root         78 Apr  5 18:15 .
drwxr-xr-x 3 root root         21 Apr  5 17:44 ..
-rw-r--r-- 1 root root 2532048896 Apr  5 21:41 pod01-psm-n3.qcow2
-rw-r--r-- 1 root root 2532048896 Apr  5 18:07 psm.qcow2
  
  

```

Run the following commands to deploy each node. PSM Node1 is used as an example below; run the same procedure on the remaining nodes as well.


```
[root@server1 1.7.0-9]# virt-install --import --name pod01-psm-n1 --virt-type kvm --cpu host-passthrough --os-variant rhel7.6 --ram 16384 --vcpu 4 --network=bridge:br11,model=virtio --disk path=/root/kvm/images/1.7.0-9/pod01-psm-n1.qcow2,format=qcow2,bus=scsi --controller scsi,model=virtio-scsi --nographics --check path_in_use=on

Starting install...
Connected to domain pod01-psm-n1
Escape character is ^]
[    0.000000] Initializing cgroup subsys cpuset
[    0.000000] Initializing cgroup subsys cpu
[    0.000000] Initializing cgroup subsys cpuacct
[    0.000000] Linux version 3.10.0-1062.4.3.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) ) #1 SMP Wed Nov 13 23:58:53 UTC 2019
[    0.000000] Command line: BOOT_IMAGE=/OS-1.7.0-9/vmlinuz0 rw rd.fstab=0 root=live:UUID=e380bc9a-9f04-4d10-9ecc-6d23d32d2fdb rd.live.dir=/OS-1.7.0-9 rd.live.squashimg=squashfs.img console=ttyS0 console=tty0 rd.live.image rd.luks=0 rd.md=0 rd.dm=0 enforcing=0 LANG=en_US.utf8 rd.writable.fsimg=1 pen.venice=OS-1.7.0-9/venice.tgz pen.naples=OS-1.7.0-9/naples_fw.tar
[    0.000000] e820: BIOS-provided physical RAM map:
	.
	.
	.
[   63.099928] Bridge firewalling registered
[   63.122358] nf_conntrack version 0.5.0 (65536 buckets, 262144 max)
[   63.233383] Netfilter messages via NETLINK v0.30.
[   63.237292] ctnetlink v0.93: registering with nfnetlink.
[   63.282430] IPv6: ADDRCONF(NETDEV_UP): docker0: link is not ready

CentOS Linux 7 (Core)
Kernel 3.10.0-1062.4.3.el7.x86_64 on an x86_64

localhost login:
  
  

```

The default login User ID and password are `root` and `centos`.
Next, use the `config_PSM_networking.py` utility to configure the network settings for each PSM node. (The command `config_PSM_networking.py -h` will show all its parameters.)


```
[root@server1]# config_PSM_networking.py -hostname pod01-psm-n1 -password mypassword  -addrtype static -ipaddr 10.29.11.21 -netmask 255.255.255.0 -gateway 10.29.11.1 -dns 10.29.5.9,8.8.8.8
  
  

```

***Note:*** *In the example above, replace* `mypassword` *with one conforming to the appropriate robustness and security guidelines for the site.*
Exit the console of the VM (typically by pressing `Ctrl+]` ).  Next, set the VM to autostart in case the CentOS server is rebooted.


```
[root@server11]# virsh autostart pod01-psm-n1
Domain pod01-psm-n1 marked as autostarted
  
  

```

  
  
[^kix.1vrgibiaznp5]: <font size='1'>If the mapping between a Host and a DSC needs to be changed within a Host object, the following procedure must be used:</font>

    1. <font size='1'>Delete any existing Workloads on the Host.</font>
    1. <font size='1'>Delete the Host object.</font>
    1. <font size='1'>Recreate the Host object with the desired DSC attributes.</font>
  
  
  
[^kix.fxq3mgc0p6ri]: <font size='1'>Refer to the Release Notes for the number of NetworkSecurityPolicy objects supported.</font>

  
  
[^kix.d4eualpq40uf]: <font size='1'>The PSM can be installed as a single VM for test and evaluation purposes.</font>

  
  
[^kix.wtyixwbt1ao]: <font size='1'>A PSM controlling DSC cards running on ESXi hosts that are hosting the PSM itself is currently not a supported feature.</font>

  
  
[^kix.fqfckeknj8li]: <font size='1'>In this release, certificates are self-signed, triggering a warning. This will be changed to authority-signed in a future release.</font>

  
  
[^kix.ir2uxzv8uqo4]: Note that the authentication information in the body of the POST request includes the indication of a `tenant`. Although the current version assumes that a single tenant (called `default`) is using the PSM and the DSCs it manages, the system has been designed to support multi-tenancy. The `default` tenant is reflected in some of the API payloads as well as certain object paths that include tenant `default`. Multi-tenancy support will be made available in future releases.
